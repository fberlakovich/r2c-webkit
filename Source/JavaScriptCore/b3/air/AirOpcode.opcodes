# Copyright (C) 2015 Apple Inc. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS''
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
# BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
# THE POSSIBILITY OF SUCH DAMAGE.

# Syllabus:
#
# Roles and types:
# U:G => use of a general-purpose register or value
# D:G => def of a general-purpose register or value
# UD:G => use and def of a general-purpose register or value
# UA:G => UseAddr (see comment in Arg.h)
# U:F => use of a float register or value
# D:F => def of a float register or value
# UD:F => use and def of a float register or value
#
# Argument kinds:
# Tmp => temporary or register
# Imm => 32-bit immediate int
# Imm64 => TrustedImm64
# Addr => address as temporary/register+offset
# Index => BaseIndex address
# Abs => AbsoluteAddress
#
# The parser views these things as keywords, and understands that they fall into two distinct classes
# of things. So, although this file uses a particular indentation style, none of the whitespace or
# even newlines are meaningful to the parser. For example, you could write:
#
# Foo42 U:G, UD:F Imm, Tmp Addr, Tmp
#
# And the parser would know that this is the same as:
#
# Foo42 U:G, UD:F
#     Imm, Tmp
#     Addr, Tmp
#
# I.e. a two-form instruction that uses a GPR or an int immediate and uses+defs a float register.
#
# Any opcode or opcode form can be preceded with an architecture list, which restricts the opcode to the
# union of those architectures. For example, if this is the only overload of the opcode, then it makes the
# opcode only available on x86_64:
#
# x86_64: Fuzz UD:G, D:G
#     Tmp, Tmp
#     Tmp, Addr
#
# But this only restricts the two-operand form, the other form is allowed on all architectures:
#
# x86_64: Fuzz UD:G, D:G
#     Tmp, Tmp
#     Tmp, Addr
# Fuzz UD:G, D:G, U:F
#     Tmp, Tmp, Tmp
#     Tmp, Addr, Tmp
#
# And you can also restrict individual forms:
#
# Thingy UD:G, D:G
#     Tmp, Tmp
#     arm64: Tmp, Addr
#
# Additionally, you can have an intersection between the architectures of the opcode overload and the
# form. In this example, the version that takes an address is only available on armv7 while the other
# versions are available on armv7 or x86_64:
#
# x86_64 armv7: Buzz U:G, UD:F
#     Tmp, Tmp
#     Imm, Tmp
#     armv7: Addr, Tmp
#
# Finally, you can specify architectures using helpful architecture groups. Here are all of the
# architecture keywords that we support:
#
# x86: means x86-32 or x86-64.
# x86_32: means just x86-32.
# x86_64: means just x86-64.
# arm: means armv7 or arm64.
# armv7: means just armv7.
# arm64: means just arm64.
# 32: means x86-32 or armv7.
# 64: means x86-64 or arm64.

# Note that the opcodes here have a leading capital (Add32) but must correspond to MacroAssembler
# API that has a leading lower-case (add32).

Nop

Add32 U:G, UD:G
    Tmp, Tmp
    x86: Imm, Addr
    Imm, Tmp
    x86: Addr, Tmp
    x86: Tmp, Addr

Add32 U:G, U:G, D:G
    Imm, Tmp, Tmp
    Tmp, Tmp, Tmp

64: Add64 U:G, UD:G
    Tmp, Tmp
    x86: Imm, Addr
    Imm, Tmp
    x86: Addr, Tmp
    x86: Tmp, Addr

64: Add64 U:G, U:G, D:G
    Imm, Tmp, Tmp
    Tmp, Tmp, Tmp

AddDouble U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

AddFloat U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

Sub32 U:G, UD:G
    Tmp, Tmp
    x86: Imm, Addr
    Imm, Tmp
    x86: Addr, Tmp
    x86: Tmp, Addr

64: Sub64 U:G, UD:G
    Tmp, Tmp
    x86: Imm, Addr
    Imm, Tmp
    x86: Addr, Tmp
    x86: Tmp, Addr

SubDouble U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

SubFloat U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

Neg32 UD:G
    Tmp
    Addr

64: Neg64 UD:G
    Tmp

Mul32 U:G, UD:G
    Tmp, Tmp
    x86: Addr, Tmp

Mul32 U:G, U:G, D:G
    Imm, Tmp, Tmp

64: Mul64 U:G, UD:G
    Tmp, Tmp

MulDouble U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

MulFloat U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

DivDouble U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

DivFloat U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

x86: X86ConvertToDoubleWord32 U:G, D:G
    Tmp*, Tmp*

x86_64: X86ConvertToQuadWord64 U:G, D:G
    Tmp*, Tmp*

x86: X86Div32 UD:G, UD:G, U:G
    Tmp*, Tmp*, Tmp

x86_64: X86Div64 UD:G, UD:G, U:G
    Tmp*, Tmp*, Tmp

Lea UA:G, D:G
    Addr, Tmp

And32 U:G, UD:G
    Tmp, Tmp
    Imm, Tmp
    x86: Tmp, Addr
    x86: Addr, Tmp
    x86: Imm, Addr

64: And64 U:G, UD:G
    Tmp, Tmp
    Imm, Tmp

AndDouble U:F, UD:F
    Tmp, Tmp

AndFloat U:F, UD:F
    Tmp, Tmp

Lshift32 U:G, UD:G
    Tmp*, Tmp
    Imm, Tmp

64: Lshift64 U:G, UD:G
    Tmp*, Tmp
    Imm, Tmp

Rshift32 U:G, UD:G
    Tmp*, Tmp
    Imm, Tmp

64: Rshift64 U:G, UD:G
    Tmp*, Tmp
    Imm, Tmp

Urshift32 U:G, UD:G
    Tmp*, Tmp
    Imm, Tmp

64: Urshift64 U:G, UD:G
    Tmp*, Tmp
    Imm, Tmp

Or32 U:G, UD:G
    Tmp, Tmp
    Imm, Tmp
    x86: Tmp, Addr
    x86: Addr, Tmp
    x86: Imm, Addr

64: Or64 U:G, UD:G
    Tmp, Tmp
    Imm, Tmp

Xor32 U:G, UD:G
    Tmp, Tmp
    Imm, Tmp
    x86: Tmp, Addr
    x86: Addr, Tmp
    x86: Imm, Addr

64: Xor64 U:G, UD:G
    Tmp, Tmp
    x86: Tmp, Addr
    Imm, Tmp

Not32 UD:G
    Tmp
    x86: Addr

64: Not64 UD:G
    Tmp
    x86: Addr

CeilDouble U:F, UD:F
    Tmp, Tmp
    Addr, Tmp

CeilFloat U:F, UD:F
    Tmp, Tmp
    Addr, Tmp

SqrtDouble U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

SqrtFloat U:F, UD:F
    Tmp, Tmp
    x86: Addr, Tmp

ConvertInt32ToDouble U:G, D:F
    Tmp, Tmp
    x86: Addr, Tmp

64: ConvertInt64ToDouble U:G, D:F
    Tmp, Tmp

CountLeadingZeros32 U:G, D:G
    Tmp, Tmp
    x86: Addr, Tmp

64: CountLeadingZeros64 U:G, D:G
    Tmp, Tmp
    x86: Addr, Tmp

ConvertDoubleToFloat U:F, D:F
    Tmp, Tmp
    x86: Addr, Tmp

ConvertFloatToDouble U:F, D:F
    Tmp, Tmp
    x86: Addr, Tmp

# Note that Move operates over the full register size, which is either 32-bit or 64-bit depending on
# the platform. I'm not entirely sure that this is a good thing; it might be better to just have a
# Move64 instruction. OTOH, our MacroAssemblers already have this notion of "move()" that basically
# means movePtr.
Move U:G, D:G
    Tmp, Tmp
    Imm, Tmp as signExtend32ToPtr
    Imm64, Tmp
    Addr, Tmp as loadPtr # This means that "Move Addr, Tmp" is code-generated as "load" not "move".
    Index, Tmp as loadPtr
    Tmp, Addr as storePtr
    Tmp, Index as storePtr
    Imm, Addr as storePtr

Move32 U:G, D:G
    Tmp, Tmp as zeroExtend32ToPtr
    Addr, Tmp as load32
    Index, Tmp as load32
    Tmp, Addr as store32
    Tmp, Index as store32
    Imm, Addr as store32
    Imm, Index as store32

SignExtend32ToPtr U:G, D:G
    Tmp, Tmp

ZeroExtend8To32 U:G, D:G
    Tmp, Tmp
    Addr, Tmp as load8
    Index, Tmp as load8

SignExtend8To32 U:G, D:G
    Tmp, Tmp
    x86: Addr, Tmp as load8SignedExtendTo32
    Index, Tmp as load8SignedExtendTo32

ZeroExtend16To32 U:G, D:G
    Tmp, Tmp
    Addr, Tmp as load16
    Index, Tmp as load16

SignExtend16To32 U:G, D:G
    Tmp, Tmp
    Addr, Tmp as load16SignedExtendTo32
    Index, Tmp as load16SignedExtendTo32

MoveFloat U:F, D:F
    Tmp, Tmp as moveDouble
    Addr, Tmp as loadFloat
    Index, Tmp as loadFloat
    Tmp, Addr as storeFloat
    Tmp, Index as storeFloat

MoveDouble U:F, D:F
    Tmp, Tmp
    Addr, Tmp as loadDouble
    Index, Tmp as loadDouble
    Tmp, Addr as storeDouble
    Tmp, Index as storeDouble

MoveZeroToDouble D:F
    Tmp

64: Move64ToDouble U:G, D:F
    Tmp, Tmp
    Addr, Tmp as loadDouble
    Index, Tmp as loadDouble

MoveInt32ToPacked U:G, D:F
    Tmp, Tmp
    Addr, Tmp as loadFloat
    Index, Tmp as loadFloat

64: MoveDoubleTo64 U:F, D:G
    Tmp, Tmp
    Addr, Tmp as load64
    Index, Tmp as load64

MovePackedToInt32 U:F, D:G
    Tmp, Tmp
    Addr, Tmp as load32
    Index, Tmp as load32

Load8 U:G, D:G
    Addr, Tmp
    Index, Tmp

Store8 U:G, D:G
    Tmp, Index
    Tmp, Addr
    Imm, Index
    Imm, Addr

Load8SignedExtendTo32 U:G, D:G
    Addr, Tmp
    Index, Tmp

Load16 U:G, D:G
    Addr, Tmp
    Index, Tmp

Load16SignedExtendTo32 U:G, D:G
    Addr, Tmp
    Index, Tmp

Compare32 U:G, U:G, U:G, D:G
    RelCond, Tmp, Tmp, Tmp
    RelCond, Tmp, Imm, Tmp

64: Compare64 U:G, U:G, U:G, D:G
    RelCond, Tmp, Imm, Tmp
    RelCond, Tmp, Tmp, Tmp

Test32 U:G, U:G, U:G, D:G
    x86: ResCond, Addr, Imm, Tmp
    ResCond, Tmp, Tmp, Tmp

64: Test64 U:G, U:G, U:G, D:G
    ResCond, Tmp, Imm, Tmp
    ResCond, Tmp, Tmp, Tmp

# Note that branches have some logic in AirOptimizeBlockOrder.cpp. If you add new branches, please make sure
# you opt them into the block order optimizations.

Branch8 U:G, U:G, U:G /branch
    x86: RelCond, Addr, Imm
    x86: RelCond, Index, Imm

Branch32 U:G, U:G, U:G /branch
    x86: RelCond, Addr, Imm
    RelCond, Tmp, Tmp
    RelCond, Tmp, Imm
    x86: RelCond, Tmp, Addr
    x86: RelCond, Addr, Tmp
    x86: RelCond, Index, Imm

64: Branch64 U:G, U:G, U:G /branch
    RelCond, Tmp, Tmp
    x86: RelCond, Tmp, Addr
    x86: RelCond, Addr, Tmp
    x86: RelCond, Index, Tmp

BranchTest8 U:G, U:G, U:G /branch
    x86: ResCond, Addr, Imm
    x86: ResCond, Index, Imm

BranchTest32 U:G, U:G, U:G /branch
    ResCond, Tmp, Tmp
    ResCond, Tmp, Imm
    x86: ResCond, Addr, Imm
    x86: ResCond, Index, Imm

# Warning: forms that take an immediate will sign-extend their immediate. You probably want
# BranchTest32 in most cases where you use an immediate.
64: BranchTest64 U:G, U:G, U:G /branch
    ResCond, Tmp, Tmp
    ResCond, Tmp, Imm
    x86: ResCond, Addr, Imm
    x86: ResCond, Addr, Tmp
    x86: ResCond, Index, Imm

BranchDouble U:G, U:F, U:F /branch
    DoubleCond, Tmp, Tmp

BranchFloat U:G, U:F, U:F /branch
    DoubleCond, Tmp, Tmp

BranchAdd32 U:G, U:G, UD:G /branch
    ResCond, Tmp, Tmp
    ResCond, Imm, Tmp
    x86: ResCond, Imm, Addr
    x86: ResCond, Tmp, Addr
    x86: ResCond, Addr, Tmp

64: BranchAdd64 U:G, U:G, UD:G /branch
    ResCond, Imm, Tmp
    ResCond, Tmp, Tmp

BranchMul32 U:G, U:G, UD:G /branch
    ResCond, Tmp, Tmp
    x86: ResCond, Addr, Tmp

BranchMul32 U:G, U:G, U:G, D:G /branch
    ResCond, Tmp, Imm, Tmp

64: BranchMul64 U:G, U:G, UD:G /branch
    ResCond, Tmp, Tmp

BranchSub32 U:G, U:G, UD:G /branch
    ResCond, Tmp, Tmp
    ResCond, Imm, Tmp
    x86: ResCond, Imm, Addr
    x86: ResCond, Tmp, Addr
    x86: ResCond, Addr, Tmp

64: BranchSub64 U:G, U:G, UD:G /branch
    ResCond, Imm, Tmp
    ResCond, Tmp, Tmp

BranchNeg32 U:G, UD:G /branch
    ResCond, Tmp

64: BranchNeg64 U:G, UD:G /branch
    ResCond, Tmp

MoveConditionally32 U:G, U:G, U:G, U:G, UD:G
    RelCond, Tmp, Tmp, Tmp, Tmp

64: MoveConditionally64 U:G, U:G, U:G, U:G, UD:G
    RelCond, Tmp, Tmp, Tmp, Tmp

MoveConditionallyTest32 U:G, U:G, U:G, U:G, UD:G
    ResCond, Tmp, Tmp, Tmp, Tmp
    ResCond, Tmp, Imm, Tmp, Tmp

64: MoveConditionallyTest64 U:G, U:G, U:G, U:G, UD:G
    ResCond, Tmp, Tmp, Tmp, Tmp
    ResCond, Tmp, Imm, Tmp, Tmp

MoveConditionallyDouble U:G, U:F, U:F, U:G, UD:G
    DoubleCond, Tmp, Tmp, Tmp, Tmp

MoveConditionallyFloat U:G, U:F, U:F, U:G, UD:G
    DoubleCond, Tmp, Tmp, Tmp, Tmp

MoveDoubleConditionally32 U:G, U:G, U:G, U:F, UD:F
    RelCond, Tmp, Tmp, Tmp, Tmp

64: MoveDoubleConditionally64 U:G, U:G, U:G, U:F, UD:F
    RelCond, Tmp, Tmp, Tmp, Tmp

MoveDoubleConditionallyTest32 U:G, U:G, U:G, U:F, UD:F
    ResCond, Tmp, Tmp, Tmp, Tmp
    ResCond, Tmp, Imm, Tmp, Tmp

64: MoveDoubleConditionallyTest64 U:G, U:G, U:G, U:F, UD:F
    ResCond, Tmp, Tmp, Tmp, Tmp
    ResCond, Tmp, Imm, Tmp, Tmp

MoveDoubleConditionallyDouble U:G, U:F, U:F, U:F, UD:F
    DoubleCond, Tmp, Tmp, Tmp, Tmp

MoveDoubleConditionallyFloat U:G, U:F, U:F, U:F, UD:F
    DoubleCond, Tmp, Tmp, Tmp, Tmp

Jump /branch

Ret /terminal

Oops /terminal

# Air allows for exotic behavior. A Patch's behavior is determined entirely by the Special operand,
# which must be the first operand.
special Patch

